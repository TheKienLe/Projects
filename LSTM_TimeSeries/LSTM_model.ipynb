{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=104, batch_first=True)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=104, out_features=52),\n",
    "            nn.Linear(in_features=52, out_features=26),\n",
    "            nn.Linear(in_features=26, out_features=1)\n",
    "        )\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.lstm(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=104, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=104, hidden_size=52, batch_first=True)\n",
    "        self.fc_layers = nn.Linear(in_features=52, out_features=1)\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.lstm1(x)\n",
    "        x = self.Tanh(x)\n",
    "        x, (_, _) = self.lstm2(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = F.mse_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "                 \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        loss, scores , y = self._common_step(batch, batch_idx)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.model(x)\n",
    "        loss = self.loss_fn(scores, y[:,0].reshape(1, 1))\n",
    "        return loss, scores, y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1897, 0.0038, 0.0307, 0.2818, 0.1922, 0.2937, 0.0000, 0.0451, 0.2248,\n",
       "        0.2868, 0.4458, 0.4796, 0.4746, 0.4546, 0.5272, 0.3262, 0.4590, 0.4847,\n",
       "        0.5930, 0.6243, 0.4984, 0.4878, 0.4734, 0.1616, 0.0770, 0.2786, 0.0463,\n",
       "        0.2292, 0.1703, 0.0182, 0.0776, 0.0407, 0.2154, 0.2655, 0.2561, 0.2279,\n",
       "        0.4922, 0.4364, 0.5529, 0.5623, 0.3907, 0.4696, 0.4433, 0.3431, 0.5441,\n",
       "        0.3181, 0.3338, 0.4101, 0.4208, 0.3319, 0.4033, 0.4847, 0.3776, 0.1916,\n",
       "        0.2185, 0.4696, 0.3801, 0.4815, 0.1879, 0.2329, 0.4126, 0.4746, 0.6337,\n",
       "        0.6675, 0.6625, 0.6425, 0.7151, 0.5141, 0.6468, 0.6725, 0.7808, 0.8121,\n",
       "        0.6863, 0.6756, 0.6612, 0.3494, 0.2649, 0.4665, 0.2342, 0.4170, 0.3582,\n",
       "        0.2060, 0.2655, 0.2286, 0.4033, 0.4534, 0.4440, 0.4158, 0.6800, 0.6243,\n",
       "        0.7408, 0.7502, 0.5786, 0.6575, 0.6312, 0.5310, 0.7320, 0.5059, 0.5216,\n",
       "        0.5980, 0.6086, 0.5197, 0.5911, 0.6725, 0.5654, 0.3795, 0.4064, 0.6575,\n",
       "        0.5679, 0.6694, 0.3757, 0.4208, 0.6005, 0.6625, 0.8215, 0.8554, 0.8503,\n",
       "        0.8303, 0.9029, 0.7019, 0.8347, 0.8604, 0.9687, 1.0000, 0.8741, 0.8635,\n",
       "        0.8491, 0.5373, 0.4527, 0.6544, 0.4220, 0.6049, 0.5460, 0.3939, 0.4534,\n",
       "        0.4164, 0.5911, 0.6412, 0.6318, 0.6036, 0.8679, 0.8121, 0.9286, 0.9380,\n",
       "        0.7664, 0.8453, 0.8190, 0.7188, 0.9198, 0.6938, 0.7095, 0.7858, 0.7965,\n",
       "        0.7076, 0.7790, 0.8604, 0.7533, 0.5673, 0.5942, 0.8453, 0.7558, 0.8572,\n",
       "        0.5636, 0.6086, 0.7884, 0.8503, 1.0094, 1.0432, 1.0382, 1.0182, 1.0908,\n",
       "        0.8898, 1.0225, 1.0482, 1.1565, 1.1879, 1.0620, 1.0513, 1.0369, 0.7251,\n",
       "        0.6406, 0.8422, 0.6099, 0.7927, 0.7339, 0.5817, 0.6412, 0.6043, 0.7790,\n",
       "        0.8291, 0.8197, 0.7915, 1.0557, 1.0000, 1.1165, 1.1259, 0.9543, 1.0332,\n",
       "        1.0069, 0.9067, 1.1077, 0.8817, 0.8973, 0.9737, 0.9843])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_construct(sale_data, time_step=3, sku_list= [\"SKU 1st\", \"SKU 2nd\", \"SKU 3rd\"]):\n",
    "\n",
    "    len_data = len(sale_data[sku_list[0]]) - 3 \n",
    "    data = np.empty([len(sku_list), len_data, time_step+1])\n",
    "\n",
    "    for sku_index, sku_val in enumerate(sku_list):\n",
    "        \n",
    "        len_data = len(sale_data[sku_val]) - time_step \n",
    "        sku_data = np.array(sale_data[sku_val]).reshape(-1, 1)\n",
    "\n",
    "        # Normalizing data\n",
    "        normalizer = MinMaxScaler().fit(sku_data[:-52])\n",
    "        sku_data = normalizer.transform(sku_data).flatten()\n",
    "\n",
    "        # Merging data\n",
    "        x = np.array([sku_data[i:i+time_step] for i in range(len_data)])\n",
    "        y = sku_data[time_step:].reshape(len_data,1)\n",
    "\n",
    "        # Check data dimensions\n",
    "        if len(sku_list) > 1: \n",
    "            data[sku_index, :, :] = np.concatenate((x, y), axis=1)\n",
    "        else:\n",
    "            data = np.concatenate((x, y), axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class SaleDataset(Dataset):\n",
    "    def __init__(self, excel_file, sku_list, time_step=3):\n",
    "        \"\"\"Initializes instance of class StudentsPerformanceDataset.\n",
    "        Args:\n",
    "            excel_file (str): Path to the csv file with the sale data.\n",
    "        \"\"\"\n",
    "\n",
    "        sale_data = pd.read_excel(excel_file, \n",
    "                     sheet_name= sku_list,\n",
    "                     usecols=\"D\",\n",
    "                     dtype=np.float32,\n",
    "                     )\n",
    "        \n",
    "        self.data = torch.Tensor(data_construct(sale_data, time_step, sku_list)).clone().detach()\n",
    "        if len(sku_list) > 1:\n",
    "            self.data = torch.permute(self.data, (1, 2, 0))\n",
    "        \n",
    "            # Save target and predictors\n",
    "            self.x = self.data[:, :time_step, :]\n",
    "            self.y = self.data[:, -1, :]\n",
    "        else:\n",
    "            self.x = self.data[:, :time_step]\n",
    "            self.y = self.data[:, -1]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return self.x[idx].clone().detach(), self.y[idx].clone().detach()\n",
    "    \n",
    "excel_file =r\"C:\\Users\\Dave\\Documents\\GitHub\\Projects\\LSTM_TimeSeries\\dataset\\Sale_data.xlsm\"\n",
    "sku_list = [\"SKU 1st\"]\n",
    "time_step = 3\n",
    "a = SaleDataset(excel_file, sku_list, time_step)\n",
    "a.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4064, 0.5602],\n",
       "         [0.6575, 0.5074],\n",
       "         [0.5679, 0.6805]]),\n",
       " tensor([0.6694, 0.5334]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SaleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers, sku_list, time_step):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.sku_list = sku_list\n",
    "        self.time_step = time_step\n",
    "\n",
    "    # def prepare_data(self):\n",
    "        \n",
    "\n",
    "    def setup(self, stage):\n",
    "        entire_dataset = SaleDataset(self.data_dir, self.sku_list, self.time_step)\n",
    "        test_size = 52\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = Subset(entire_dataset, list(range(len(entire_dataset)-test_size)))\n",
    "            self.train_ds, self.val_ds = random_split(self.train_ds, \n",
    "                                                    [int(len(self.train_ds)*0.85), len(self.train_ds) - int(len(self.train_ds)*0.85)])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_ds =  Subset(entire_dataset, list(range(len(entire_dataset)-test_size, len(entire_dataset))))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            # persistent_workers=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            # persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            # persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    \n",
    "excel_file =r\"C:\\Users\\Dave\\Documents\\GitHub\\Projects\\LSTM_TimeSeries\\dataset\\Sale_data.xlsm\"\n",
    "sku_list = [\"SKU 1st\", \"SKU 2nd\"]\n",
    "dm = SaleDataModule(data_dir=excel_file, batch_size=1, num_workers=0, sku_list=sku_list, time_step=time_step)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "dm.train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 33\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | LSTM2 | 80.8 K\n",
      "--------------------------------\n",
      "80.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.8 K    Total params\n",
      "0.323     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\Dave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 130/130 [00:01<00:00, 81.94it/s, v_num=114]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 130/130 [00:01<00:00, 81.29it/s, v_num=114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 217.61it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     validation_loss       0.014160525985062122\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 52/52 [00:00<00:00, 291.19it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss          0.015026570297777653\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(33)\n",
    "\n",
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "num_workers = 0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "time_step = 3 \n",
    "\n",
    "# data\n",
    "excel_file =r\"C:\\Users\\Dave\\Documents\\GitHub\\Projects\\LSTM_TimeSeries\\dataset\\Sale_data.xlsm\"\n",
    "sku_list = [\"SKU 1st\", \"SKU 2nd\", \"SKU 3rd\"]\n",
    "input_size = 3 if len(sku_list) == 1 else 9\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Data\n",
    "    dm = SaleDataModule(data_dir=excel_file, batch_size=batch_size, num_workers=num_workers, sku_list=sku_list, time_step=time_step)\n",
    "    # Initialize network\n",
    "    model = NN(LSTM2(input_size=input_size))\n",
    "\n",
    "    trainer = pl.Trainer(accelerator=\"cpu\", devices=1, min_epochs=1, max_epochs=num_epochs, log_every_n_steps=10)\n",
    "    trainer.fit(model, dm)\n",
    "    trainer.validate(model, dm)\n",
    "    trainer.test(model, dm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
